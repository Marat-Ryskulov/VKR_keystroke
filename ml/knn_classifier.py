# ml/knn_classifier.py - K-–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä

import numpy as np
from typing import Tuple, Optional, List
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import pickle

from config import KNN_NEIGHBORS, MODELS_DIR
import os

class KNNAuthenticator:
    """KNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –¥–∏–Ω–∞–º–∏–∫–µ –Ω–∞–∂–∞—Ç–∏–π"""
    

    def __init__(self, n_neighbors: int = KNN_NEIGHBORS):
        self.n_neighbors = min(n_neighbors, 3)  # –ú–∞–∫—Å–∏–º—É–º 3 —Å–æ—Å–µ–¥–∞
        self.model = KNeighborsClassifier(
            n_neighbors=self.n_neighbors,
            metric='euclidean',
            weights='distance',     # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –≤–∞–∂–Ω–µ–µ
            algorithm='ball_tree'   # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        )
        self.is_trained = False
        self.normalization_stats = None
        self.training_data = None
        
    def train(self, X_positive: np.ndarray, X_negative: np.ndarray = None) -> Tuple[bool, float]:
        """–û–±—É—á–µ–Ω–∏–µ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º –±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤"""
        n_samples = len(X_positive)
        if n_samples < 5:
            return False, 0.0
    
        print(f"\nüéØ –°–¢–†–û–ì–û–ï –û–ë–£–ß–ï–ù–ò–ï")
        print(f"–¢–≤–æ–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_samples}")
    
        self.training_data = X_positive.copy()
    
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if X_negative is None or len(X_negative) == 0:
            X_negative = self._generate_synthetic_negatives(X_positive)
    
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –±–µ—Ä–µ–º –ù–ê–ú–ù–û–ì–û –º–µ–Ω—å—à–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 3:1 –≤ –ø–æ–ª—å–∑—É —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        neg_count = max(n_samples // 4, 5)  # –ú–∏–Ω–∏–º—É–º 5, –Ω–æ –æ–±—ã—á–Ω–æ –≤ 4 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
    
        if len(X_negative) > neg_count:
            # –ë–µ—Ä–µ–º –°–ê–ú–´–ï –î–ê–õ–ï–ö–ò–ï –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances(X_negative, X_positive)
            min_distances = np.min(distances, axis=1)
            farthest_indices = np.argsort(min_distances)[-neg_count:]
            X_negative = X_negative[farthest_indices]
    
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(X_positive)} –¢–í–û–ò–• vs {len(X_negative)} –ß–£–ñ–ò–•")
        print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(X_positive)/(len(X_positive)+len(X_negative))*100:.0f}% —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {np.min(euclidean_distances(X_negative, X_positive)):.2f}")
    
        # –û–±—É—á–µ–Ω–∏–µ
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(len(X_positive)), np.zeros(len(X_negative))])
    
        self.model.fit(X, y)
        self.is_trained = True
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        train_accuracy = self.model.score(X, y)
        print(f"Train accuracy: {train_accuracy:.3f}")

        self.test_data = {
            'X_positive': X_positive.copy(),
            'X_negative': X_negative.copy() if X_negative is not None else None,
            'y_positive': np.ones(len(X_positive)),
            'y_negative': np.zeros(len(X_negative)) if X_negative is not None else None
        }
    
        return True, train_accuracy
    
    def authenticate(self, features: np.ndarray, threshold: float = 0.5) -> Tuple[bool, float]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        """
        if not self.is_trained:
            return False, 0.0
    
        print(f"\n=== –ù–ê–ß–ê–õ–û –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===")
        print(f"–í—Ö–æ–¥—è—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features}")
    
        # 1. –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ KNN
        probabilities = self.model.predict_proba(features.reshape(1, -1))[0]
        print(f"KNN probabilities: {probabilities}")
        print(f"KNN classes: {self.model.classes_}")
    
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        knn_probability = 0.0
        if len(probabilities) > 1 and 1.0 in self.model.classes_:
            class_1_index = list(self.model.classes_).index(1.0)
            knn_probability = probabilities[class_1_index]
    
        print(f"KNN auth probability: {knn_probability:.3f}")
    
        # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        distance_score = 0.0
        if hasattr(self, 'training_data') and self.training_data is not None:
            from sklearn.metrics.pairwise import euclidean_distances
        
            X_positive = self.training_data
            distances = euclidean_distances(features.reshape(1, -1), X_positive)[0]
        
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            min_distance = np.min(distances)
            mean_distance = np.mean(distances)
            median_distance = np.median(distances)
        
            # k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
            k = min(5, len(distances))
            nearest_distances = np.sort(distances)[:k]
            avg_nearest = np.mean(nearest_distances)
        
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
            if len(X_positive) > 1:
                train_distances = euclidean_distances(X_positive, X_positive)
                train_distances = train_distances[train_distances > 0]
                mean_train_distance = np.mean(train_distances)
                std_train_distance = np.std(train_distances)
                median_train_distance = np.median(train_distances)
            else:
                mean_train_distance = 1.0
                std_train_distance = 0.5
                median_train_distance = 1.0
        
            print(f"\n–ê–ù–ê–õ–ò–ó –†–ê–°–°–¢–û–Ø–ù–ò–ô:")
            print(f"  Min distance: {min_distance:.4f}")
            print(f"  Mean distance: {mean_distance:.4f}")
            print(f"  Median distance: {median_distance:.4f}")
            print(f"  Avg nearest {k}: {avg_nearest:.4f}")
            print(f"  Train mean: {mean_train_distance:.4f}")
            print(f"  Train std: {std_train_distance:.4f}")
            print(f"  Train median: {median_train_distance:.4f}")
        
            # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            scores = []
        
            # –û—Ü–µ–Ω–∫–∞ 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            norm_min = min_distance / (mean_train_distance + 1e-6)
            score1 = max(0, 1.0 - norm_min * 0.8)
            scores.append(('min_dist', score1))
        
            # –û—Ü–µ–Ω–∫–∞ 2: –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö
            norm_nearest = avg_nearest / (mean_train_distance + 1e-6)
            score2 = max(0, 1.0 - norm_nearest * 0.6)
            scores.append(('nearest_avg', score2))
        
            # –û—Ü–µ–Ω–∫–∞ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–µ–¥–∏–∞–Ω–æ–π
            norm_median = min_distance / (median_train_distance + 1e-6)
            score3 = max(0, 1.0 - norm_median * 0.7)
            scores.append(('median_comp', score3))
        
            # –û—Ü–µ–Ω–∫–∞ 4: Z-score –∞–Ω–∞–ª–∏–∑
            if std_train_distance > 0:
                z_score = abs(min_distance - mean_train_distance) / std_train_distance
                score4 = max(0, 1.0 - z_score * 0.3)
            else:
                score4 = score1
            scores.append(('z_score', score4))
        
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            distance_score = np.mean([s[1] for s in scores])
        
            print(f"  Distance scores: {scores}")
            print(f"  Combined distance score: {distance_score:.3f}")
    
        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å)
        feature_score = 1.0
        if hasattr(self, 'training_data') and self.training_data is not None:
            X_positive = self.training_data
        
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            train_mean = np.mean(X_positive, axis=0)
            train_std = np.std(X_positive, axis=0)
        
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫
            feature_penalties = []
            feature_names = ['avg_dwell', 'std_dwell', 'avg_flight', 'std_flight', 'speed', 'total_time']
        
            for i, (feat_val, train_m, train_s, name) in enumerate(zip(features, train_mean, train_std, feature_names)):
                if train_s > 0:
                    z_score = abs(feat_val - train_m) / train_s
                    penalty = min(0.3, z_score * 0.1)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ 30%
                else:
                    penalty = 0.0
            
                feature_penalties.append(penalty)
                print(f"  {name}: val={feat_val:.4f}, train_mean={train_m:.4f}, z_score={(abs(feat_val - train_m) / (train_s + 1e-6)):.2f}, penalty={penalty:.3f}")
        
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ—ã
            total_penalty = sum(feature_penalties) / len(feature_penalties)
            feature_score = max(0.1, 1.0 - total_penalty)
        
            print(f"  Feature analysis score: {feature_score:.3f}")
    
        # 4. –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        if len(self.training_data) >= 30:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ –¥–æ–≤–µ—Ä—è–µ–º ML –º–æ–¥–µ–ª–∏
            weights = {'knn': 0.5, 'distance': 0.3, 'features': 0.2}
        elif len(self.training_data) >= 15:
            # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º
            weights = {'knn': 0.4, 'distance': 0.4, 'features': 0.2}
        else:
            # –î–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ –¥–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è–º
            weights = {'knn': 0.2, 'distance': 0.6, 'features': 0.2}
    
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final_probability = (
            weights['knn'] * knn_probability +
            weights['distance'] * distance_score +
            weights['features'] * feature_score
        )
    
        print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ï –û–¶–ï–ù–ö–ò:")
        print(f"  KNN: {knn_probability:.3f} (–≤–µ—Å: {weights['knn']})")
        print(f"  Distance: {distance_score:.3f} (–≤–µ—Å: {weights['distance']})")
        print(f"  Features: {feature_score:.3f} (–≤–µ—Å: {weights['features']})")
        print(f"  Final probability: {final_probability:.3f}")
        print(f"  Threshold: {threshold}")
    
        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
        is_authenticated = final_probability >= threshold
    
        print(f"  –†–ï–ó–£–õ–¨–¢–ê–¢: {'–ü–†–ò–ù–Ø–¢' if is_authenticated else '–û–¢–ö–õ–û–ù–ï–ù'}")
        print(f"=== –ö–û–ù–ï–¶ –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===\n")
    
        return is_authenticated, final_probability
    
    def _generate_synthetic_negatives(self, X_positive: np.ndarray, factor: float = 1.5) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –æ—Ç–ª–∏—á–∞—é—â–∏—Ö—Å—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        n_samples = len(X_positive)
        n_features = X_positive.shape[1]
    
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¢–í–û–ò –¥–∞–Ω–Ω—ã–µ
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        min_vals = np.min(X_positive, axis=0)
        max_vals = np.max(X_positive, axis=0)
    
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –¢–í–û–ò–• –î–ê–ù–ù–´–•:")
        print(f"  –£–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–ª–∞–≤–∏—à: {mean[0]*1000:.1f} ¬± {std[0]*1000:.1f} –º—Å")
        print(f"  –í—Ä–µ–º—è –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏: {mean[2]*1000:.1f} ¬± {std[2]*1000:.1f} –º—Å")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å –ø–µ—á–∞—Ç–∏: {mean[4]:.1f} ¬± {std[4]:.1f} –∫–ª/—Å")
        print(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è: {mean[5]:.1f} ¬± {std[5]:.1f} —Å–µ–∫")
    
        synthetic_samples = []
    
        # 1. –ß–ï–†–ï–ü–ê–•–ò (–≤ 20 —Ä–∞–∑ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
        print("–°–æ–∑–¥–∞–µ–º –ß–ï–†–ï–ü–ê–•...")
        for i in range(n_samples // 2):
            sample = np.array([
                mean[0] * np.random.uniform(15, 25),    # —É–¥–µ—Ä–∂–∞–Ω–∏–µ –≤ 15-25 —Ä–∞–∑ –¥–æ–ª—å—à–µ
                mean[1] * np.random.uniform(10, 20),    # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∞–Ω–∏—è  
                mean[2] * np.random.uniform(20, 40),    # –ø–∞—É–∑—ã –≤ 20-40 —Ä–∞–∑ –¥–æ–ª—å—à–µ
                mean[3] * np.random.uniform(15, 30),    # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—É–∑
                mean[4] * np.random.uniform(0.02, 0.08), # —Å–∫–æ—Ä–æ—Å—Ç—å –≤ 12-50 —Ä–∞–∑ –º–µ–¥–ª–µ–Ω–Ω–µ–µ
                mean[5] * np.random.uniform(15, 30)     # –≤—Ä–µ–º—è –≤ 15-30 —Ä–∞–∑ –±–æ–ª—å—à–µ
            ])
            synthetic_samples.append(sample)
    
        # 2. –ì–ï–ü–ê–†–î–´ (–≤ 20 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ)  
        print("–°–æ–∑–¥–∞–µ–º –ì–ï–ü–ê–†–î–û–í...")
        for i in range(n_samples // 2):
            sample = np.array([
                mean[0] * np.random.uniform(0.02, 0.08), # —É–¥–µ—Ä–∂–∞–Ω–∏–µ –≤ 12-50 —Ä–∞–∑ –∫–æ—Ä–æ—á–µ
                mean[1] * np.random.uniform(0.05, 0.15), # –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                mean[2] * np.random.uniform(0.01, 0.05), # –ø–∞—É–∑—ã –≤ 20-100 —Ä–∞–∑ –∫–æ—Ä–æ—á–µ
                mean[3] * np.random.uniform(0.02, 0.10), # –æ—á–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω–æ
                mean[4] * np.random.uniform(15, 40),     # —Å–∫–æ—Ä–æ—Å—Ç—å –≤ 15-40 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ
                mean[5] * np.random.uniform(0.05, 0.20)  # –≤—Ä–µ–º—è –≤ 5-20 —Ä–∞–∑ –º–µ–Ω—å—à–µ
            ])
            synthetic_samples.append(sample)
    
        # 3. –†–û–ë–û–¢–´ (–Ω—É–ª–µ–≤–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)
        print("–°–æ–∑–¥–∞–µ–º –†–û–ë–û–¢–û–í...")
        for i in range(n_samples // 3):
            base_dwell = np.random.uniform(0.01, 0.30)
            base_flight = np.random.uniform(0.01, 0.50)
            sample = np.array([
                base_dwell,                             # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                base_dwell * 0.001,                     # –ø–æ—á—Ç–∏ –Ω–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                base_flight,                            # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—É–∑—ã  
                base_flight * 0.001,                    # –ø–æ—á—Ç–∏ –Ω–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                1.0 / (base_dwell + base_flight + 0.01), # –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                43 * (base_dwell + base_flight)         # —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è 43 —Å–∏–º–≤–æ–ª–æ–≤
            ])
            synthetic_samples.append(sample)
    
        # 4. –•–ê–û–° (–æ–≥—Ä–æ–º–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)
        print("–°–æ–∑–¥–∞–µ–º –•–ê–û–°...")
        for i in range(n_samples // 3):
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ç–≤–æ–∏—Ö, –Ω–æ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –û–ì–†–û–ú–ù–ê–Ø
            base_dwell = mean[0] * np.random.uniform(0.5, 2.0)
            base_flight = mean[2] * np.random.uniform(0.3, 3.0)
        
            sample = np.array([
                base_dwell,                             # —Å—Ä–µ–¥–Ω–µ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                base_dwell * np.random.uniform(5, 15),  # –û–ì–†–û–ú–ù–ê–Ø –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∞–Ω–∏—è
                base_flight,                            # —Å—Ä–µ–¥–Ω–µ–µ –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏
                base_flight * np.random.uniform(8, 20), # –û–ì–†–û–ú–ù–ê–Ø –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—É–∑
                mean[4] * np.random.uniform(0.3, 3.0),  # –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                mean[5] * np.random.uniform(0.5, 4.0)   # –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –≤—Ä–µ–º—è
            ])
            synthetic_samples.append(sample)
    
        # 5. –ò–ù–û–ü–õ–ê–ù–ï–¢–Ø–ù–ï (–∏–∑ –¥—Ä—É–≥–æ–π –≤—Å–µ–ª–µ–Ω–Ω–æ–π)
        print("–°–æ–∑–¥–∞–µ–º –ò–ù–û–ü–õ–ê–ù–ï–¢–Ø–ù...")
        for i in range(n_samples // 3):
            sample = np.array([
                np.random.uniform(0.005, 3.0),          # –ª—é–±–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                np.random.uniform(0.001, 2.0),          # –ª—é–±–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                np.random.uniform(0.005, 8.0),          # –ª—é–±—ã–µ –ø–∞—É–∑—ã
                np.random.uniform(0.001, 4.0),          # –ª—é–±–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                np.random.uniform(0.1, 100.0),          # –ª—é–±–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                np.random.uniform(1.0, 200.0)           # –ª—é–±–æ–µ –≤—Ä–µ–º—è
            ])
            synthetic_samples.append(sample)
    
        result = np.array(synthetic_samples)
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(result, X_positive)
        min_distances = np.min(distances, axis=1)
    
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ù–ï–ì–ê–¢–ò–í–ù–´–• –ü–†–ò–ú–ï–†–û–í:")
        print(f"  –°–æ–∑–¥–∞–Ω–æ: {len(result)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {np.min(min_distances):.2f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.mean(min_distances):.2f}")
        print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.max(min_distances):.2f}")
    
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–æ –∫ —Ç–≤–æ–∏–º –¥–∞–Ω–Ω—ã–º
        threshold = np.mean(min_distances)  # –°—Ä–µ–¥–Ω–∏–π –ø–æ—Ä–æ–≥
        far_indices = min_distances >= threshold
        result_filtered = result[far_indices]
    
        print(f"  –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(result_filtered)} –æ–±—Ä–∞–∑—Ü–æ–≤ (—É–±—Ä–∞–ª–∏ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–∏–µ)")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {np.min(min_distances[far_indices]):.2f}")
    
        return result_filtered
    
    def save_model(self, user_id: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
    
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        model_data = {
            'model': self.model,
            'n_neighbors': self.n_neighbors,
            'normalization_stats': self.normalization_stats,
            'is_trained': self.is_trained,
            'training_data': self.training_data,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            'test_data': self.test_data  # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        }
    
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)

    @classmethod
    def load_model(cls, user_id: int) -> Optional['KNNAuthenticator']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        if not os.path.exists(model_path):
            return None
    
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
        
            authenticator = cls(n_neighbors=model_data['n_neighbors'])
            authenticator.model = model_data['model']
            authenticator.normalization_stats = model_data['normalization_stats']
            authenticator.is_trained = model_data['is_trained']
            authenticator.training_data = model_data.get('training_data', None)  # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

            authenticator.test_data = model_data.get('test_data', None)  # ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        
            return authenticator
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)"""
        if not self.is_trained:
            return []
        
        # –î–ª—è KNN –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é
        feature_names = [
            'avg_dwell_time',
            'std_dwell_time', 
            'avg_flight_time',
            'std_flight_time',
            'typing_speed',
            'total_typing_time'
        ]
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        X_train = self.model._fit_X
        
        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        variances = np.var(X_train, axis=0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        importance = variances / np.sum(variances)
        
        return list(zip(feature_names, importance))