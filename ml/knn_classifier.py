# ml/knn_classifier.py - K-–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä

import numpy as np
from typing import Tuple, Optional, List
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import pickle

from config import KNN_NEIGHBORS, MODELS_DIR
import os

class KNNAuthenticator:
    """KNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –¥–∏–Ω–∞–º–∏–∫–µ –Ω–∞–∂–∞—Ç–∏–π"""
    

    def __init__(self, n_neighbors: int = KNN_NEIGHBORS):
        self.n_neighbors = min(n_neighbors, 3)  # –ú–∞–∫—Å–∏–º—É–º 3 —Å–æ—Å–µ–¥–∞
        self.model = KNeighborsClassifier(
            n_neighbors=self.n_neighbors,
            metric='euclidean',
            weights='distance',     # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –≤–∞–∂–Ω–µ–µ
            algorithm='ball_tree'   # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        )
        self.is_trained = False
        self.normalization_stats = None
        self.training_data = None
        
    def train(self, X_positive: np.ndarray, X_negative: np.ndarray = None) -> Tuple[bool, float]:
        """–û–±—É—á–µ–Ω–∏–µ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º –±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤"""
        n_samples = len(X_positive)
        if n_samples < 5:
            return False, 0.0
    
        print(f"\nüéØ –°–¢–†–û–ì–û–ï –û–ë–£–ß–ï–ù–ò–ï")
        print(f"–¢–≤–æ–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {n_samples}")
    
        self.training_data = X_positive.copy()
    
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if X_negative is None or len(X_negative) == 0:
            X_negative = self._generate_synthetic_negatives(X_positive)
    
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –±–µ—Ä–µ–º –ù–ê–ú–ù–û–ì–û –º–µ–Ω—å—à–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 3:1 –≤ –ø–æ–ª—å–∑—É —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        neg_count = max(n_samples // 4, 5)  # –ú–∏–Ω–∏–º—É–º 5, –Ω–æ –æ–±—ã—á–Ω–æ –≤ 4 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
    
        if len(X_negative) > neg_count:
            # –ë–µ—Ä–µ–º –°–ê–ú–´–ï –î–ê–õ–ï–ö–ò–ï –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            from sklearn.metrics.pairwise import euclidean_distances
            distances = euclidean_distances(X_negative, X_positive)
            min_distances = np.min(distances, axis=1)
            farthest_indices = np.argsort(min_distances)[-neg_count:]
            X_negative = X_negative[farthest_indices]
    
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(X_positive)} –¢–í–û–ò–• vs {len(X_negative)} –ß–£–ñ–ò–•")
        print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(X_positive)/(len(X_positive)+len(X_negative))*100:.0f}% —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {np.min(euclidean_distances(X_negative, X_positive)):.2f}")
    
        # –û–±—É—á–µ–Ω–∏–µ
        X = np.vstack([X_positive, X_negative])
        y = np.hstack([np.ones(len(X_positive)), np.zeros(len(X_negative))])
    
        self.model.fit(X, y)
        self.is_trained = True
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        train_accuracy = self.model.score(X, y)
        print(f"Train accuracy: {train_accuracy:.3f}")

        self.test_data = {
            'X_positive': X_positive.copy(),
            'X_negative': X_negative.copy() if X_negative is not None else None,
            'y_positive': np.ones(len(X_positive)),
            'y_negative': np.zeros(len(X_negative)) if X_negative is not None else None
        }
    
        return True, train_accuracy
    
    def authenticate(self, features: np.ndarray, threshold: float = 0.5, verbose: bool = False) -> Tuple[bool, float, dict]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (—Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Ñ–∏–Ω–∞–ª—å–Ω–∞—è_—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –¥–µ—Ç–∞–ª—å–Ω–∞—è_—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        if not self.is_trained:
            return False, 0.0, {}

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ features - —ç—Ç–æ 1D –º–∞—Å—Å–∏–≤
        if features.ndim > 1:
            features = features.flatten()

        if verbose:
            print(f"\n=== –ù–ê–ß–ê–õ–û –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===")
            print(f"–í—Ö–æ–¥—è—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features}")

        # 1. –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ KNN
        features_reshaped = features.reshape(1, -1)
        probabilities = self.model.predict_proba(features_reshaped)[0]

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        knn_probability = 0.0
        if len(probabilities) > 1 and 1.0 in self.model.classes_:
            class_1_index = list(self.model.classes_).index(1.0)
            knn_probability = probabilities[class_1_index]

        # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        distance_score = 0.0
        distance_details = {}

        if hasattr(self, 'training_data') and self.training_data is not None:
            from sklearn.metrics.pairwise import euclidean_distances
    
            X_positive = self.training_data
            distances = euclidean_distances(features_reshaped, X_positive)[0]
    
            min_distance = np.min(distances)
            mean_distance = np.mean(distances)
    
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if len(X_positive) > 1:
                train_distances = euclidean_distances(X_positive, X_positive)
                train_distances = train_distances[train_distances > 0]
                mean_train_distance = np.mean(train_distances)
                std_train_distance = np.std(train_distances)
            else:
                mean_train_distance = 1.0
                std_train_distance = 0.5
    
            # –û—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            norm_min = min_distance / (mean_train_distance + 1e-6)
            distance_score = max(0, 1.0 - norm_min * 0.8)
    
            distance_details = {
                'min_distance': min_distance,
                'mean_distance': mean_distance,
                'mean_train_distance': mean_train_distance,
                'normalized_distance': norm_min
            }

        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_score = 1.0
        feature_details = {}

        if hasattr(self, 'training_data') and self.training_data is not None:
            X_positive = self.training_data
            train_mean = np.mean(X_positive, axis=0)
            train_std = np.std(X_positive, axis=0)
    
            feature_penalties = []
            feature_names = ['avg_dwell', 'std_dwell', 'avg_flight', 'std_flight', 'speed', 'total_time']
    
            for i, (feat_val, train_m, train_s, name) in enumerate(zip(features, train_mean, train_std, feature_names)):
                if train_s > 0:
                    z_score_val = abs(feat_val - train_m) / train_s
                    if hasattr(z_score_val, '__len__'):
                        z_score_val = float(z_score_val)
                    penalty = min(0.3, z_score_val * 0.1)
                else:
                    penalty = 0.0
                    z_score_val = 0.0
        
                feature_penalties.append(penalty)
                feature_details[name] = {
                    'value': float(feat_val),
                    'train_mean': float(train_m),
                    'train_std': float(train_s),
                    'z_score': float(z_score_val),
                    'penalty': penalty
                }
    
            total_penalty = sum(feature_penalties) / len(feature_penalties)
            feature_score = max(0.1, 1.0 - total_penalty)

        # 4. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        if len(self.training_data) >= 30:
            weights = {'knn': 0.5, 'distance': 0.3, 'features': 0.2}
        elif len(self.training_data) >= 15:
            weights = {'knn': 0.4, 'distance': 0.4, 'features': 0.2}
        else:
            weights = {'knn': 0.2, 'distance': 0.6, 'features': 0.2}

        final_probability = (
            weights['knn'] * knn_probability +
            weights['distance'] * distance_score +
            weights['features'] * feature_score
        )

        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
        is_authenticated = final_probability >= threshold

        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        detailed_stats = {
            'knn_confidence': knn_probability,
            'distance_score': distance_score,
            'feature_score': feature_score,
            'final_confidence': final_probability,
            'threshold': threshold,
            'weights': weights,
            'distance_details': distance_details,
            'feature_details': feature_details,
            'training_samples': len(self.training_data) if hasattr(self, 'training_data') else 0
        }

        if verbose:
            print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ï –û–¶–ï–ù–ö–ò:")
            print(f"  KNN: {knn_probability:.3f} (–≤–µ—Å: {weights['knn']})")
            print(f"  Distance: {distance_score:.3f} (–≤–µ—Å: {weights['distance']})")
            print(f"  Features: {feature_score:.3f} (–≤–µ—Å: {weights['features']})")
            print(f"  Final: {final_probability:.3f} (–ø–æ—Ä–æ–≥: {threshold})")
            print(f"  –†–ï–ó–£–õ–¨–¢–ê–¢: {'–ü–†–ò–ù–Ø–¢' if is_authenticated else '–û–¢–ö–õ–û–ù–ï–ù'}")
            print(f"=== –ö–û–ù–ï–¶ –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò ===\n")

        return is_authenticated, final_probability, detailed_stats
    
    def _generate_synthetic_negatives(self, X_positive: np.ndarray, factor: float = 1.5) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        n_samples = len(X_positive)
        n_features = X_positive.shape[1]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¢–í–û–ò –¥–∞–Ω–Ω—ã–µ
        mean = np.mean(X_positive, axis=0)
        std = np.std(X_positive, axis=0)
        min_vals = np.min(X_positive, axis=0)
        max_vals = np.max(X_positive, axis=0)

        print(f"\nüîç –ê–ù–ê–õ–ò–ó –¢–í–û–ò–• –î–ê–ù–ù–´–•:")
        print(f"  –£–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–ª–∞–≤–∏—à: {mean[0]*1000:.1f} ¬± {std[0]*1000:.1f} –º—Å")
        print(f"  –í—Ä–µ–º—è –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏: {mean[2]*1000:.1f} ¬± {std[2]*1000:.1f} –º—Å")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å –ø–µ—á–∞—Ç–∏: {mean[4]:.1f} ¬± {std[4]:.1f} –∫–ª/—Å")
        print(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è: {mean[5]:.1f} ¬± {std[5]:.1f} —Å–µ–∫")

        synthetic_samples = []

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ë–ª–∏–∑–∫–∏–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (25%) - —Ç—Ä—É–¥–Ω–µ–µ –≤—Å–µ–≥–æ –æ—Ç–ª–∏—á–∏—Ç—å
        close_count = n_samples // 4
        print(f"–°–æ–∑–¥–∞–µ–º {close_count} –ë–õ–ò–ó–ö–ò–• –ö–û–ù–ö–£–†–ï–ù–¢–û–í...")
        for i in range(close_count):
            # –ù–µ–±–æ–ª—å—à–∏–µ, –Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–ª–∏—á–∏—è –≤ 1-2 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
            sample = mean.copy()
        
            # –í—ã–±–∏—Ä–∞–µ–º 1-2 –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
            features_to_change = np.random.choice(6, size=np.random.randint(1, 3), replace=False)
        
            for feat_idx in features_to_change:
                if feat_idx in [0, 1]:  # –≤—Ä–µ–º—è —É–¥–µ—Ä–∂–∞–Ω–∏—è
                    factor = np.random.choice([0.7, 1.4])  # –±—ã—Å—Ç—Ä–µ–µ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                elif feat_idx in [2, 3]:  # –≤—Ä–µ–º—è –º–µ–∂–¥—É –∫–ª–∞–≤–∏—à–∞–º–∏
                    factor = np.random.choice([0.6, 1.6])  # –±—ã—Å—Ç—Ä–µ–µ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                elif feat_idx == 4:  # —Å–∫–æ—Ä–æ—Å—Ç—å
                    factor = np.random.choice([0.8, 1.3])  # –Ω–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                else:  # –æ–±—â–µ–µ –≤—Ä–µ–º—è
                    factor = np.random.choice([0.75, 1.35])
            
                sample[feat_idx] = mean[feat_idx] * factor
        
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            noise = np.random.normal(0, std * 0.3)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.1)  # –Ω–µ –¥–∞–µ–º —Å—Ç–∞—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–º–∏
            synthetic_samples.append(sample)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –î—Ä—É–≥–æ–π —Å—Ç–∏–ª—å –ø–µ—á–∞—Ç–∏ (40%)
        different_style_count = int(n_samples * 0.4)
        print(f"–°–æ–∑–¥–∞–µ–º {different_style_count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –î–†–£–ì–ò–ú –°–¢–ò–õ–ï–ú...")
        for i in range(different_style_count):
            # –ë–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω—ã–µ, –Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è
            if np.random.random() < 0.5:
                # "–û—Ö–æ—Ç–Ω–∏–∫–∏ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ" - –±—ã—Å—Ç—Ä—ã–µ –∏ —Ç–æ—á–Ω—ã–µ
                style_factors = np.array([
                    np.random.uniform(0.4, 0.8),     # –±—ã—Å—Ç—Ä–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(0.5, 0.9),     # —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(0.3, 0.7),     # –±—ã—Å—Ç—Ä—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(0.4, 0.8),     # —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(1.2, 2.5),     # –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(0.4, 0.8)      # –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                ])
            else:
                # "–†–∞–∑–º—ã—à–ª—è—é—â–∏–µ –ø–µ—á–∞—Ç–∞—é—â–∏–µ" - –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∏ –≤–¥—É–º—á–∏–≤—ã–µ
                style_factors = np.array([
                    np.random.uniform(1.3, 2.5),     # –¥–æ–ª–≥–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(1.1, 2.0),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(1.5, 3.5),     # –¥–æ–ª–≥–∏–µ –ø–∞—É–∑—ã
                    np.random.uniform(1.2, 2.8),     # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—É–∑—ã
                    np.random.uniform(0.3, 0.8),     # –Ω–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(1.3, 2.5)      # –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                ])
        
            sample = mean * style_factors
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —à—É–º
            noise = np.random.normal(0, std * 0.5)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.05)
            synthetic_samples.append(sample)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∏–º–∏—Ç–∞—Ç–æ—Ä—ã (25%) - –ø—ã—Ç–∞—é—Ç—Å—è –ø–æ–¥—Ä–∞–∂–∞—Ç—å, –Ω–æ –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ
        adaptive_count = int(n_samples * 0.25)
        print(f"–°–æ–∑–¥–∞–µ–º {adaptive_count} –ê–î–ê–ü–¢–ò–í–ù–´–• –ò–ú–ò–¢–ê–¢–û–†–û–í...")
        for i in range(adaptive_count):
            # –ë–µ—Ä–µ–º –≤–∞—à–∏ —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –æ—Å–Ω–æ–≤—É, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
            sample = mean.copy()
        
            # –ò–º–∏—Ç–∞—Ç–æ—Ä –Ω–µ –º–æ–∂–µ—Ç —Ç–æ—á–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            # –û–Ω –±–ª–∏–∑–æ–∫ –ø–æ 3-4 –ø—Ä–∏–∑–Ω–∞–∫–∞–º, –Ω–æ –æ—à–∏–±–∞–µ—Ç—Å—è –≤ 2-3
            success_rate = np.random.uniform(0.6, 0.8)  # —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∏–º–∏—Ç–∞—Ü–∏–∏
        
            for j in range(6):
                if np.random.random() > success_rate:
                    # –û—à–∏–±–∫–∞ –≤ –∏–º–∏—Ç–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
                    if j in [0, 2]:  # –≤—Ä–µ–º–µ–Ω–∞ - —Å–ª–æ–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ
                        error_factor = np.random.uniform(0.6, 1.7)
                    else:  # –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                        error_factor = np.random.uniform(0.7, 1.5)
                    sample[j] = mean[j] * error_factor
                else:
                    # –£—Å–ø–µ—à–Ω–∞—è –∏–º–∏—Ç–∞—Ü–∏—è - –±–ª–∏–∑–∫–æ –∫ –≤–∞—à–∏–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                    sample[j] = mean[j] * np.random.uniform(0.9, 1.1)
        
            # –ò–º–∏—Ç–∞—Ç–æ—Ä—ã –æ–±—ã—á–Ω–æ –º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã
            instability_noise = np.random.normal(0, std * 0.8)
            sample = sample + instability_noise
            sample = np.maximum(sample, mean * 0.1)
            synthetic_samples.append(sample)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ (10%) - —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—â–∏–µ—Å—è
        extreme_count = n_samples - close_count - different_style_count - adaptive_count
        print(f"–°–æ–∑–¥–∞–µ–º {extreme_count} –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–• –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        for i in range(extreme_count):
            if np.random.random() < 0.5:
                # –°—É–ø–µ—Ä-–±—ã—Å—Ç—Ä—ã–µ
                extreme_factors = np.array([
                    np.random.uniform(0.1, 0.5),     # –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(0.2, 0.6),     # –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    np.random.uniform(0.05, 0.3),    # –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(0.1, 0.5),     # —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
                    np.random.uniform(3.0, 8.0),     # –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(0.1, 0.4)      # –æ—á–µ–Ω—å –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏
                ])
            else:
                # –°—É–ø–µ—Ä-–º–µ–¥–ª–µ–Ω–Ω—ã–µ
                extreme_factors = np.array([
                    np.random.uniform(3.0, 8.0),     # –æ—á–µ–Ω—å –¥–æ–ª–≥–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ
                    np.random.uniform(2.5, 6.0),     # –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    np.random.uniform(4.0, 12.0),    # –æ—á–µ–Ω—å –¥–æ–ª–≥–∏–µ –ø–∞—É–∑—ã
                    np.random.uniform(3.0, 8.0),     # –æ—á–µ–Ω—å –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—É–∑—ã
                    np.random.uniform(0.1, 0.4),     # –æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                    np.random.uniform(3.0, 8.0)      # –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
                ])
        
            sample = mean * extreme_factors
            noise = np.random.normal(0, std * 0.3)
            sample = sample + noise
            sample = np.maximum(sample, mean * 0.01)
            synthetic_samples.append(sample)

        result = np.array(synthetic_samples)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(result, X_positive)
        min_distances = np.min(distances, axis=1)

        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –ù–ï–ì–ê–¢–ò–í–ù–´–• –ü–†–ò–ú–ï–†–û–í:")
        print(f"  –°–æ–∑–¥–∞–Ω–æ: {len(result)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {np.min(min_distances):.3f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.mean(min_distances):.3f}")
        print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {np.max(min_distances):.3f}")
    
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        very_close = np.sum(min_distances < np.std(min_distances) * 0.5)
        close = np.sum((min_distances >= np.std(min_distances) * 0.5) & 
                    (min_distances < np.mean(min_distances)))
        far = len(min_distances) - very_close - close
    
        print(f"  –û—á–µ–Ω—å –±–ª–∏–∑–∫–∏–µ (—Ç—Ä—É–¥–Ω—ã–µ): {very_close}")
        print(f"  –£–º–µ—Ä–µ–Ω–Ω–æ –±–ª–∏–∑–∫–∏–µ: {close}")
        print(f"  –î–∞–ª–µ–∫–∏–µ (–ª–µ–≥–∫–∏–µ): {far}")

        # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã (–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—à–∏–±–æ—á–Ω–æ –ø—Ä–∏–Ω—è—Ç—ã)
        distance_threshold = np.percentile(min_distances, 20)  # —É–±–∏—Ä–∞–µ–º 20% —Å–∞–º—ã—Ö –±–ª–∏–∑–∫–∏—Ö
        good_indices = min_distances >= distance_threshold
        result_filtered = result[good_indices]

        print(f"  –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–∏—Ö: {len(result_filtered)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
        if len(result_filtered) < len(result) * 0.7:  # –µ—Å–ª–∏ —É–±—Ä–∞–ª–∏ –±–æ–ª—å—à–µ 30%
            print("  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —É–±—Ä–∞–Ω–æ –º–Ω–æ–≥–æ –±–ª–∏–∑–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ - –≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è")
    
        return result_filtered
    
    def save_model(self, user_id: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
    
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        model_data = {
            'model': self.model,
            'n_neighbors': self.n_neighbors,
            'normalization_stats': self.normalization_stats,
            'is_trained': self.is_trained,
            'training_data': self.training_data,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            'test_data': self.test_data  # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        }
    
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)

    @classmethod
    def load_model(cls, user_id: int) -> Optional['KNNAuthenticator']:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞"""
        model_path = os.path.join(MODELS_DIR, f"user_{user_id}_knn.pkl")
    
        if not os.path.exists(model_path):
            return None
    
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
        
            authenticator = cls(n_neighbors=model_data['n_neighbors'])
            authenticator.model = model_data['model']
            authenticator.normalization_stats = model_data['normalization_stats']
            authenticator.is_trained = model_data['is_trained']
            authenticator.training_data = model_data.get('training_data', None)  # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

            authenticator.test_data = model_data.get('test_data', None)  # ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        
            return authenticator
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None
    
    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)"""
        if not self.is_trained:
            return []
        
        # –î–ª—è KNN –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é
        feature_names = [
            'avg_dwell_time',
            'std_dwell_time', 
            'avg_flight_time',
            'std_flight_time',
            'typing_speed',
            'total_typing_time'
        ]
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        X_train = self.model._fit_X
        
        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        variances = np.var(X_train, axis=0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        importance = variances / np.sum(variances)
        
        return list(zip(feature_names, importance))